#!/bin/bash
#SBATCH --job-name=slurm_gpu    # Job name
#SBATCH --mail-type=END,FAIL          # Mail events (NONE, BEGIN, END, FAIL, ALL)
#SBATCH --mail-user=ehar@stlawu.edu     # Where to send mail	
#SBATCH --mem=32G                       # Job memory request
#SBATCH --time=05:00:00                 # Time limit hrs:min:sec
#SBATCH --output=slurm_gpu_%j.log       # Standard output and error log
#SBATCH --cpus-per-gpu=1		# Count of CPUs allocated per GPU.
#SBATCH --gres=gpu:1			# Allocate 1 GPU card
#SBATCH -n 1				# Allocate one node


echo "Date              = $(date)"
echo "Hostname          = $(hostname -s)"
echo "Working Directory = $(pwd)"
echo ""
echo "Number of Nodes Allocated      = $SLURM_JOB_NUM_NODES"
echo "Number of Tasks Allocated      = $SLURM_NTASKS"
echo "Number of Cores/Task Allocated = $SLURM_CPUS_PER_TASK"

echo ""
echo -n "Loading module python/anaconda3....   "
module load python/anaconda3
echo "Done!"
echo ""
echo "CUDA_VISIBLE_DEVICES is set to ${CUDA_VISIBLE_DEVICES}"

echo -n "Executing program at: "
date
echo ""
python mnist_cnn.py
echo ""
echo -n "Finished program at: "
date
echo ""
